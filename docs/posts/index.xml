<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Synthesized Reality</title>
    <link>https://yuki-tkd.github.io/posts/</link>
    <description>Recent content in Posts on Synthesized Reality</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Sun, 19 Mar 2023 11:46:03 +0000</lastBuildDate><atom:link href="https://yuki-tkd.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ChatGPTにステレオ画像からDepth推定するコードを書いてもらう</title>
      <link>https://yuki-tkd.github.io/posts/chat_gpt_depth/</link>
      <pubDate>Sun, 19 Mar 2023 11:46:03 +0000</pubDate>
      
      <guid>https://yuki-tkd.github.io/posts/chat_gpt_depth/</guid>
      <description>背景 どうやらChatGPTが凄いという話をよく聞くので、自分でも凄さを体験してみた。
やったこと 無料版のChatGPT (GPT3.5) を利用して、OpenCVを使ったステレオ画像からのDepth推定を行ってもらった。データセットは、UTIAS long-term localization and mapping dataset の C00005を利用させて頂く。
ChatGPTにコードを書いて貰うのは初なので、いきなりダイレクトに聞いてみた。
凄い！！ 一発でそれっぽいコードが生成された。
でも、これはそのまま実行できない。なぜなら、ステレオカメラの内部・外部パラメータのキャリブレーションファイル calibration_file.yaml が存在しておらず、テキスト形式で与えられた内部パラメータと外部パラメータ(基線長)しかないから。
そこで、その点を指摘しつつ、テキスト形式のままパラメータを与えてみた。
パラメータの意味を理解して、コードを修正してくれた。これでDepth画像は正しく作れるようになった。
ただ、Depth画像が不可視だったので、その点を指摘した。 (自分の指示が間違っていますね。16bit pngが人間の目で見えないだけで、オーバフローは起こしていませんでした。)
上記のコードで最終的に算出されたDepth画像がこちら。
カラー画像 Depth画像 (を可視化したもの) 正しく求まっていそう。ライブラリを使ってサンプルコードくらいのものを書いてもらうという作業なら、十分依頼できそうだ。ChatGPT4は更に高性能らしいし、自分が洗練したプロンプトを入力できれば、もっと複雑なこともできるのだろう。
あとがき 月並みだが、ChatGPT以前と以後ではプログラミングという作業の形態がガラッと変わるだろう、と感じた。
数年もしないうちに、①人間がやりたいことを考える → ②人間がプロンプトを音声で発話してChatGPTがコード化 → ③結果を人間が確認。必要があれば再度変更点を伝える というスタイルが当たり前になっていくのでは？という気がしている。
したがって、下記のような仕事は真っ先にChatGPTで代替されるだろう、という危機感を感じた。
ペアプロのドライバー役 実際、ChatGPTにコードを書いてもらっているときの感覚はナビゲーターそのものだった。 リファクタリング テストの生成 一方で、&amp;ldquo;何を実現したいか&amp;rdquo; という部分が人間依存のものである以上、&amp;ldquo;指示出し&amp;rdquo; と &amp;ldquo;結果確認&amp;rdquo; は、やっぱり人間が行うしかないのだろうなと感じる。
そして、人間への指示出しと同様に、その作業は誰にでも行えるものではないのだろう。 プロンプトエンジニアは職業として成立するのか？ 特定の学習済モデルだけに人生をかけて大丈夫なのか？ なんて思っていたが、実はプログラマーよりも本質的な仕事なのかも知れない。</description>
    </item>
    
    <item>
      <title>Netcode for GameObjectsで大容量データを送受信する</title>
      <link>https://yuki-tkd.github.io/posts/hello-world/</link>
      <pubDate>Sun, 12 Mar 2023 07:24:17 +0000</pubDate>
      
      <guid>https://yuki-tkd.github.io/posts/hello-world/</guid>
      <description>背景 Netcode for GameObjects(NGO) の Custom messages で2KB程度のメッセージを送ろうとしたら、Reading past the end of the buffer というエラーで送信できなかった。
原因 NGOは com.unity.transport 上に構築した UnityTransport を利用してクライアント・サーバ間の通信を行っている。どちらもUnityTransportと呼ばれていて分かりづらいが、前者はUnity本体の機能で、後者はNGOの機能。
ベースとなる com.unity.transport がUDPで通信しているため、MTU(1500 Bytes程度が一般的)を超えるPayloadを1つのPacketで送受信できないのが問題。
対策 Fragmentation という Payloadを複数のPacketに分割送信する機能 を利用する。
NGOのチュートリアルには書かれていないが、CustomMessagingManager.SendNamedMessage() の引数に NetworkDelivery というQoSを指定する項目が存在する。
指定可能な NetworkDelibery は、ここ から選択可能。
Realiable / Unreliable = メッセージの到着保証を行うかどうか Sequenced = メッセージの到着順保証を行うかどうか Fragmented = メッセージの分割送受信をサポートするかどうか 今回はメッセージを分割送信したいので、 ReliableFragmentedSequenced を選べば良い。
余談 RPC Custom Message ではなく RPC にもQoSを設定する項目はあるが、v1.2.0 の時点では Fragmentationに対応していない ようなので、RPC用途であったとしてもMTUを超えるデータ量がある場合は CustomMessageを利用することになりそう。
通信量制御 こちらの記事 に詳述されている通り、通信量が過剰になった際の対処は入っていない。大量にデータを送りすぎると NetworkManagerのQueueが肥大化してGCにKillされてしまうので注意。</description>
    </item>
    
  </channel>
</rss>
